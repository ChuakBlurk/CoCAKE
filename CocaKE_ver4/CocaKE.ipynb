{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9000,"status":"ok","timestamp":1655905719209,"user":{"displayName":"Bruce HU","userId":"02309357718155453395"},"user_tz":-120},"id":"vZwq84Xtc1JK","outputId":"77ade2aa-8a6b-428a-9ffb-8458e6684a7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 33.9 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 68.3 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 13.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 57.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PxdHBvfLxt2P","outputId":"5de47d26-2333-46e9-a943-5f58a61e443e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655890178613,"user_tz":-120,"elapsed":367,"user":{"displayName":"Bruce HU","userId":"02309357718155453395"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["python3: can't open file 'preprocess.py': [Errno 2] No such file or directory\n"]}],"source":["!python -u preprocess.py --task \"FB15k237\" \\\n","--train-path \"data/FB15k237/train.txt\" \\\n","--valid-path \"data/FB15k237/valid.txt\" \\\n","--test-path \"data/FB15k237/test.txt\""]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Dk4WVZrxxTi","executionInfo":{"status":"ok","timestamp":1655905735252,"user_tz":-120,"elapsed":16047,"user":{"displayName":"Bruce HU","userId":"02309357718155453395"}},"outputId":"8cdedaa8-85ec-461d-fcd2-52032e888062"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fsHAPIyscjw_","outputId":"5286bc55-b057-4981-da7c-72e419dcd728"},"outputs":[{"output_type":"stream","name":"stdout","text":["[2022-06-22 13:53:31,170 INFO] Load 14541 entities from /content/drive/MyDrive/CocaKE_ver4/data/FB15k237/entities.json\n","[2022-06-22 13:53:31,170 INFO] Triplets path: ['/content/drive/MyDrive/CocaKE_ver4/data/FB15k237/train.txt.json']\n","[2022-06-22 13:53:33,224 INFO] Triplet statistics: 474 relations, 544230 triplets\n","[2022-06-22 13:53:33,224 INFO] Start to build link graph from /content/drive/MyDrive/CocaKE_ver4/data/FB15k237/train.txt.json\n","[2022-06-22 13:53:34,003 INFO] Done build link graph with 14505 nodes\n","[2022-06-22 13:53:34,092 INFO] Use 1 gpus for training\n","[2022-06-22 13:53:34,368 INFO] Build tokenizer from bert-base-uncased\n","[2022-06-22 13:53:34,368 INFO] => creating model\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[2022-06-22 13:53:36,086 INFO] CustomBertModel(\n","  (hr_bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (tail_bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n",")\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[2022-06-22 13:53:40,593 INFO] log_inv_t: 1.0\n","[2022-06-22 13:53:40,593 INFO] hr_bert.embeddings.word_embeddings.weight: 23440896\n","[2022-06-22 13:53:40,593 INFO] hr_bert.embeddings.position_embeddings.weight: 393216\n","[2022-06-22 13:53:40,593 INFO] hr_bert.embeddings.token_type_embeddings.weight: 1536\n","[2022-06-22 13:53:40,593 INFO] hr_bert.embeddings.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,594 INFO] hr_bert.embeddings.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,594 INFO] hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.0.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.0.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.0.output.dense.bias: 768\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.0.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.0.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.1.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.1.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.1.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.1.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.1.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.1.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.1.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,595 INFO] hr_bert.encoder.layer.1.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.1.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.1.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.1.output.dense.bias: 768\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.1.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.1.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.2.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.2.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.2.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.2.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,596 INFO] hr_bert.encoder.layer.2.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.output.dense.bias: 768\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.2.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,597 INFO] hr_bert.encoder.layer.3.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,598 INFO] hr_bert.encoder.layer.3.output.dense.bias: 768\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.3.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.3.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,599 INFO] hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.4.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.4.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.4.output.dense.bias: 768\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.4.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.4.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.5.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.5.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.5.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.5.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.5.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.5.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.5.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.5.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,600 INFO] hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.5.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.5.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.5.output.dense.bias: 768\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.5.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.5.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.6.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.6.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.6.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.6.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.6.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,601 INFO] hr_bert.encoder.layer.6.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.6.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.6.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.6.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.6.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.6.output.dense.bias: 768\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.6.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.6.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.7.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.7.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,602 INFO] hr_bert.encoder.layer.7.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.output.dense.bias: 768\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,603 INFO] hr_bert.encoder.layer.7.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,604 INFO] hr_bert.encoder.layer.8.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.8.output.dense.bias: 768\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.8.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.8.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,605 INFO] hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.9.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.9.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.9.output.dense.bias: 768\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.9.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.9.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.10.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.10.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.10.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.10.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.10.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.10.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.10.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,606 INFO] hr_bert.encoder.layer.10.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.10.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.10.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.10.output.dense.bias: 768\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.10.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.10.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.11.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.11.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.11.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.11.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,607 INFO] hr_bert.encoder.layer.11.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.output.dense.bias: 768\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,608 INFO] hr_bert.encoder.layer.11.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,608 INFO] hr_bert.pooler.dense.weight: 589824\n","[2022-06-22 13:53:40,608 INFO] hr_bert.pooler.dense.bias: 768\n","[2022-06-22 13:53:40,609 INFO] tail_bert.embeddings.word_embeddings.weight: 23440896\n","[2022-06-22 13:53:40,609 INFO] tail_bert.embeddings.position_embeddings.weight: 393216\n","[2022-06-22 13:53:40,609 INFO] tail_bert.embeddings.token_type_embeddings.weight: 1536\n","[2022-06-22 13:53:40,609 INFO] tail_bert.embeddings.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,609 INFO] tail_bert.embeddings.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,609 INFO] tail_bert.encoder.layer.0.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,609 INFO] tail_bert.encoder.layer.0.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,609 INFO] tail_bert.encoder.layer.0.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,623 INFO] tail_bert.encoder.layer.0.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,624 INFO] tail_bert.encoder.layer.0.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,624 INFO] tail_bert.encoder.layer.0.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,625 INFO] tail_bert.encoder.layer.0.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,625 INFO] tail_bert.encoder.layer.0.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,626 INFO] tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,626 INFO] tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,626 INFO] tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,626 INFO] tail_bert.encoder.layer.0.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,626 INFO] tail_bert.encoder.layer.0.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,626 INFO] tail_bert.encoder.layer.0.output.dense.bias: 768\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.0.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.0.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,627 INFO] tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.1.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.1.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.1.output.dense.bias: 768\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.1.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.1.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.2.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.2.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.2.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.2.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.2.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.2.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.2.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,628 INFO] tail_bert.encoder.layer.2.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.2.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.2.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.2.output.dense.bias: 768\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.2.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.2.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.3.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.3.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.3.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.3.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,629 INFO] tail_bert.encoder.layer.3.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.output.dense.bias: 768\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.3.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.4.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,630 INFO] tail_bert.encoder.layer.4.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.output.dense.bias: 768\n","[2022-06-22 13:53:40,631 INFO] tail_bert.encoder.layer.4.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.4.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,632 INFO] tail_bert.encoder.layer.5.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.5.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.5.output.dense.bias: 768\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.5.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.5.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.6.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.6.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.6.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.6.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.6.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.6.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.6.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.6.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,633 INFO] tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.6.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.6.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.6.output.dense.bias: 768\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.6.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.6.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.7.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.7.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.7.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.7.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,634 INFO] tail_bert.encoder.layer.7.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.output.dense.bias: 768\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.7.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.8.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,635 INFO] tail_bert.encoder.layer.8.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.output.dense.bias: 768\n","[2022-06-22 13:53:40,636 INFO] tail_bert.encoder.layer.8.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.8.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,637 INFO] tail_bert.encoder.layer.9.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.9.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.9.output.dense.bias: 768\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.9.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.9.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.10.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.10.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.10.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.10.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.10.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.10.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.10.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.10.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,638 INFO] tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.10.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.10.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.10.output.dense.bias: 768\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.10.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.10.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.11.attention.self.query.weight: 589824\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.11.attention.self.query.bias: 768\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.11.attention.self.key.weight: 589824\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.11.attention.self.key.bias: 768\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.11.attention.self.value.weight: 589824\n","[2022-06-22 13:53:40,639 INFO] tail_bert.encoder.layer.11.attention.self.value.bias: 768\n","[2022-06-22 13:53:40,640 INFO] tail_bert.encoder.layer.11.attention.output.dense.weight: 589824\n","[2022-06-22 13:53:40,640 INFO] tail_bert.encoder.layer.11.attention.output.dense.bias: 768\n","[2022-06-22 13:53:40,640 INFO] tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,640 INFO] tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,640 INFO] tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296\n","[2022-06-22 13:53:40,640 INFO] tail_bert.encoder.layer.11.intermediate.dense.bias: 3072\n","[2022-06-22 13:53:40,640 INFO] tail_bert.encoder.layer.11.output.dense.weight: 2359296\n","[2022-06-22 13:53:40,640 INFO] tail_bert.encoder.layer.11.output.dense.bias: 768\n","[2022-06-22 13:53:40,640 INFO] tail_bert.encoder.layer.11.output.LayerNorm.weight: 768\n","[2022-06-22 13:53:40,640 INFO] tail_bert.encoder.layer.11.output.LayerNorm.bias: 768\n","[2022-06-22 13:53:40,640 INFO] tail_bert.pooler.dense.weight: 589824\n","[2022-06-22 13:53:40,640 INFO] tail_bert.pooler.dense.bias: 768\n","[2022-06-22 13:53:40,640 INFO] Number of parameters: 218.0M\n","[2022-06-22 13:53:40,680 INFO] In test mode: False\n","[2022-06-22 13:53:41,170 INFO] Load 272115 examples from /content/drive/MyDrive/CocaKE_ver4/data/FB15k237/train.txt.json\n","[2022-06-22 13:53:42,671 INFO] In test mode: False\n","[2022-06-22 13:53:42,707 INFO] Load 17535 examples from /content/drive/MyDrive/CocaKE_ver4/data/FB15k237/valid.txt.json\n","[2022-06-22 13:53:42,747 INFO] Total training steps: 612258, warmup steps: 400\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","[2022-06-22 13:53:42,748 INFO] Args={\n","    \"pretrained_model\": \"bert-base-uncased\",\n","    \"task\": \"FB15k237\",\n","    \"train_path\": \"/content/drive/MyDrive/CocaKE_ver4/data/FB15k237/train.txt.json\",\n","    \"valid_path\": \"/content/drive/MyDrive/CocaKE_ver4/data/FB15k237/valid.txt.json\",\n","    \"model_dir\": \"/content/drive/MyDrive/CocaKE_ver4/ouput\",\n","    \"warmup\": 400,\n","    \"max_to_keep\": 5,\n","    \"grad_clip\": 10.0,\n","    \"pooling\": \"mean\",\n","    \"dropout\": 0.1,\n","    \"use_amp\": true,\n","    \"t\": 0.05,\n","    \"use_link_graph\": true,\n","    \"eval_every_n_step\": 10000,\n","    \"pre_batch\": 2,\n","    \"pre_batch_weight\": 0.5,\n","    \"additive_margin\": 0.02,\n","    \"finetune_t\": true,\n","    \"max_num_tokens\": 50,\n","    \"use_self_negative\": true,\n","    \"commonsense_path\": \"/content/drive/MyDrive/CocaKE_ver4/data/FB15k237\",\n","    \"head_ns_cnt\": 4,\n","    \"tail_ns_cnt\": 4,\n","    \"workers\": 4,\n","    \"epochs\": 18,\n","    \"batch_size\": 16,\n","    \"lr\": 1e-05,\n","    \"lr_scheduler\": \"linear\",\n","    \"weight_decay\": 0.0001,\n","    \"print_freq\": 20,\n","    \"seed\": null,\n","    \"is_test\": false,\n","    \"rerank_n_hop\": 2,\n","    \"neighbor_weight\": 0.0,\n","    \"eval_model_path\": \"\",\n","    \"cake_gamma\": 9.0,\n","    \"cake_adversial_weight\": 10.0\n","}\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","[2022-06-22 13:53:50,276 INFO] Epoch: [0][    0/34014]\tLoss 14.32 (14.32)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.00)\tAcc@3  31.25 ( 31.25)\tcake 6.882 (6.882)\tsimkgc 7.441 (7.441)\n","[2022-06-22 13:54:34,848 INFO] Epoch: [0][   20/34014]\tLoss 15.73 (15.04)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.30)\tAcc@3  12.50 ( 24.11)\tcake 6.941 (6.92)\tsimkgc 8.792 (8.118)\n","[2022-06-22 13:55:17,142 INFO] Epoch: [0][   40/34014]\tLoss 15.06 (14.81)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.30)\tAcc@3  18.75 ( 28.51)\tcake 7.106 (6.996)\tsimkgc 7.954 (7.815)\n","[2022-06-22 13:55:59,098 INFO] Epoch: [0][   60/34014]\tLoss 14.87 (14.73)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.41)\tAcc@3  18.75 ( 29.20)\tcake 7.689 (7.135)\tsimkgc 7.179 (7.598)\n","[2022-06-22 13:56:41,038 INFO] Epoch: [0][   80/34014]\tLoss 14.44 (14.64)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.46)\tAcc@3  12.50 ( 27.39)\tcake 8.066 (7.307)\tsimkgc 6.371 (7.332)\n","[2022-06-22 13:57:23,390 INFO] Epoch: [0][  100/34014]\tLoss 14.63 (14.62)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.43)\tAcc@3  12.50 ( 25.74)\tcake 8.53 (7.498)\tsimkgc 6.095 (7.124)\n","[2022-06-22 13:58:05,069 INFO] Epoch: [0][  120/34014]\tLoss 14.18 (14.6)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.62)\tAcc@3  25.00 ( 24.79)\tcake 8.604 (7.673)\tsimkgc 5.571 (6.923)\n","[2022-06-22 13:58:47,508 INFO] Epoch: [0][  140/34014]\tLoss 14.48 (14.55)\tInvT  20.00 ( 20.00)\tAcc@1   0.00 (  0.58)\tAcc@3  18.75 ( 25.00)\tcake 8.537 (7.798)\tsimkgc 5.944 (6.75)\n","[2022-06-22 13:59:30,520 INFO] Epoch: [0][  160/34014]\tLoss 13.67 (14.47)\tInvT  20.00 ( 20.00)\tAcc@1   6.25 (  0.62)\tAcc@3  25.00 ( 25.35)\tcake 8.337 (7.875)\tsimkgc 5.33 (6.594)\n","[2022-06-22 14:00:13,455 INFO] Epoch: [0][  180/34014]\tLoss 12.92 (14.36)\tInvT  19.99 ( 20.00)\tAcc@1   6.25 (  0.76)\tAcc@3  43.75 ( 26.35)\tcake 8.189 (7.918)\tsimkgc 4.728 (6.439)\n","[2022-06-22 14:00:55,392 INFO] Epoch: [0][  200/34014]\tLoss 13.48 (14.24)\tInvT  19.99 ( 20.00)\tAcc@1   0.00 (  0.72)\tAcc@3  43.75 ( 27.67)\tcake 8.082 (7.938)\tsimkgc 5.401 (6.303)\n","[2022-06-22 14:01:38,234 INFO] Epoch: [0][  220/34014]\tLoss 12.2 (14.12)\tInvT  19.99 ( 20.00)\tAcc@1   0.00 (  0.79)\tAcc@3  62.50 ( 29.19)\tcake 7.961 (7.945)\tsimkgc 4.235 (6.179)\n","[2022-06-22 14:02:20,243 INFO] Epoch: [0][  240/34014]\tLoss 11.86 (13.95)\tInvT  19.99 ( 20.00)\tAcc@1  12.50 (  1.12)\tAcc@3  56.25 ( 30.94)\tcake 7.579 (7.931)\tsimkgc 4.284 (6.022)\n","[2022-06-22 14:03:02,617 INFO] Epoch: [0][  260/34014]\tLoss 10.45 (13.76)\tInvT  19.99 ( 20.00)\tAcc@1  12.50 (  1.99)\tAcc@3  68.75 ( 32.97)\tcake 7.405 (7.885)\tsimkgc 3.04 (5.871)\n","[2022-06-22 14:03:44,665 INFO] Epoch: [0][  280/34014]\tLoss 9.703 (13.54)\tInvT  19.99 ( 20.00)\tAcc@1  31.25 (  3.02)\tAcc@3  81.25 ( 35.10)\tcake 7.149 (7.827)\tsimkgc 2.554 (5.711)\n","[2022-06-22 14:04:27,236 INFO] Epoch: [0][  300/34014]\tLoss 10.69 (13.34)\tInvT  19.99 ( 20.00)\tAcc@1  12.50 (  4.13)\tAcc@3  68.75 ( 37.40)\tcake 6.711 (7.762)\tsimkgc 3.978 (5.576)\n","[2022-06-22 14:05:10,262 INFO] Epoch: [0][  320/34014]\tLoss 10.21 (13.15)\tInvT  19.99 ( 20.00)\tAcc@1  18.75 (  5.33)\tAcc@3  68.75 ( 39.37)\tcake 6.699 (7.698)\tsimkgc 3.509 (5.451)\n","[2022-06-22 14:05:52,409 INFO] Epoch: [0][  340/34014]\tLoss 9.89 (12.95)\tInvT  19.99 ( 19.99)\tAcc@1  37.50 (  6.60)\tAcc@3  68.75 ( 41.33)\tcake 6.246 (7.628)\tsimkgc 3.643 (5.318)\n","[2022-06-22 14:06:34,095 INFO] Epoch: [0][  360/34014]\tLoss 10.12 (12.76)\tInvT  19.99 ( 19.99)\tAcc@1  18.75 (  7.89)\tAcc@3  75.00 ( 42.92)\tcake 6.472 (7.559)\tsimkgc 3.646 (5.204)\n","[2022-06-22 14:07:16,237 INFO] Epoch: [0][  380/34014]\tLoss 9.593 (12.57)\tInvT  19.99 ( 19.99)\tAcc@1  37.50 (  9.33)\tAcc@3  62.50 ( 44.72)\tcake 5.899 (7.49)\tsimkgc 3.695 (5.079)\n","[2022-06-22 14:07:58,220 INFO] Epoch: [0][  400/34014]\tLoss 8.944 (12.37)\tInvT  19.99 ( 19.99)\tAcc@1  43.75 ( 10.91)\tAcc@3  81.25 ( 46.46)\tcake 5.761 (7.412)\tsimkgc 3.183 (4.956)\n","[2022-06-22 14:08:40,523 INFO] Epoch: [0][  420/34014]\tLoss 7.265 (12.18)\tInvT  19.99 ( 19.99)\tAcc@1  50.00 ( 12.29)\tAcc@3  93.75 ( 47.89)\tcake 5.693 (7.334)\tsimkgc 1.571 (4.843)\n","[2022-06-22 14:09:22,796 INFO] Epoch: [0][  440/34014]\tLoss 8.036 (12.0)\tInvT  19.99 ( 19.99)\tAcc@1  31.25 ( 13.63)\tAcc@3  75.00 ( 49.43)\tcake 5.751 (7.26)\tsimkgc 2.285 (4.74)\n","[2022-06-22 14:10:06,085 INFO] Epoch: [0][  460/34014]\tLoss 7.392 (11.83)\tInvT  19.99 ( 19.99)\tAcc@1  62.50 ( 14.85)\tAcc@3  87.50 ( 50.94)\tcake 5.503 (7.191)\tsimkgc 1.889 (4.637)\n","[2022-06-22 14:10:49,554 INFO] Epoch: [0][  480/34014]\tLoss 7.708 (11.66)\tInvT  19.99 ( 19.99)\tAcc@1  50.00 ( 16.23)\tAcc@3  93.75 ( 52.35)\tcake 5.429 (7.123)\tsimkgc 2.278 (4.537)\n","[2022-06-22 14:11:32,002 INFO] Epoch: [0][  500/34014]\tLoss 7.29 (11.51)\tInvT  19.99 ( 19.99)\tAcc@1  62.50 ( 17.45)\tAcc@3  87.50 ( 53.51)\tcake 5.525 (7.057)\tsimkgc 1.765 (4.448)\n","[2022-06-22 14:12:13,628 INFO] Epoch: [0][  520/34014]\tLoss 8.039 (11.35)\tInvT  19.98 ( 19.99)\tAcc@1  18.75 ( 18.63)\tAcc@3  81.25 ( 54.68)\tcake 5.365 (6.995)\tsimkgc 2.674 (4.358)\n","[2022-06-22 14:12:55,731 INFO] Epoch: [0][  540/34014]\tLoss 8.17 (11.22)\tInvT  19.98 ( 19.99)\tAcc@1  50.00 ( 19.69)\tAcc@3  68.75 ( 55.83)\tcake 5.54 (6.935)\tsimkgc 2.63 (4.281)\n","[2022-06-22 14:13:37,774 INFO] Epoch: [0][  560/34014]\tLoss 7.476 (11.09)\tInvT  19.98 ( 19.99)\tAcc@1  50.00 ( 20.72)\tAcc@3  93.75 ( 56.85)\tcake 5.469 (6.881)\tsimkgc 2.007 (4.204)\n","[2022-06-22 14:14:20,680 INFO] Epoch: [0][  580/34014]\tLoss 7.329 (10.96)\tInvT  19.98 ( 19.99)\tAcc@1  62.50 ( 21.90)\tAcc@3  93.75 ( 57.92)\tcake 5.46 (6.83)\tsimkgc 1.869 (4.126)\n","[2022-06-22 14:15:03,848 INFO] Epoch: [0][  600/34014]\tLoss 7.301 (10.83)\tInvT  19.98 ( 19.99)\tAcc@1  50.00 ( 23.02)\tAcc@3  93.75 ( 58.86)\tcake 5.292 (6.783)\tsimkgc 2.01 (4.052)\n","[2022-06-22 14:15:46,532 INFO] Epoch: [0][  620/34014]\tLoss 7.63 (10.72)\tInvT  19.98 ( 19.99)\tAcc@1  50.00 ( 23.95)\tAcc@3  87.50 ( 59.82)\tcake 5.249 (6.735)\tsimkgc 2.381 (3.988)\n","[2022-06-22 14:16:30,135 INFO] Epoch: [0][  640/34014]\tLoss 6.898 (10.61)\tInvT  19.98 ( 19.99)\tAcc@1  50.00 ( 24.90)\tAcc@3 100.00 ( 60.75)\tcake 5.147 (6.69)\tsimkgc 1.752 (3.92)\n","[2022-06-22 14:17:11,787 INFO] Epoch: [0][  660/34014]\tLoss 7.056 (10.5)\tInvT  19.98 ( 19.99)\tAcc@1  50.00 ( 26.00)\tAcc@3  93.75 ( 61.72)\tcake 5.232 (6.646)\tsimkgc 1.824 (3.849)\n","[2022-06-22 14:17:54,121 INFO] Epoch: [0][  680/34014]\tLoss 6.204 (10.4)\tInvT  19.98 ( 19.99)\tAcc@1  62.50 ( 26.88)\tAcc@3 100.00 ( 62.54)\tcake 5.215 (6.606)\tsimkgc 0.9897 (3.792)\n","[2022-06-22 14:18:36,331 INFO] Epoch: [0][  700/34014]\tLoss 7.041 (10.3)\tInvT  19.98 ( 19.99)\tAcc@1  37.50 ( 27.60)\tAcc@3  87.50 ( 63.27)\tcake 5.216 (6.564)\tsimkgc 1.825 (3.737)\n","[2022-06-22 14:19:18,975 INFO] Epoch: [0][  720/34014]\tLoss 6.334 (10.21)\tInvT  19.98 ( 19.99)\tAcc@1  62.50 ( 28.46)\tAcc@3 100.00 ( 63.91)\tcake 5.168 (6.523)\tsimkgc 1.166 (3.683)\n","[2022-06-22 14:20:00,742 INFO] Epoch: [0][  740/34014]\tLoss 5.704 (10.11)\tInvT  19.98 ( 19.99)\tAcc@1  81.25 ( 29.24)\tAcc@3 100.00 ( 64.69)\tcake 5.079 (6.484)\tsimkgc 0.6247 (3.624)\n","[2022-06-22 14:20:43,306 INFO] Epoch: [0][  760/34014]\tLoss 6.355 (10.02)\tInvT  19.98 ( 19.99)\tAcc@1  75.00 ( 29.99)\tAcc@3 100.00 ( 65.41)\tcake 5.026 (6.446)\tsimkgc 1.329 (3.574)\n","[2022-06-22 14:21:27,312 INFO] Epoch: [0][  780/34014]\tLoss 6.562 (9.93)\tInvT  19.97 ( 19.99)\tAcc@1  50.00 ( 30.79)\tAcc@3  87.50 ( 66.06)\tcake 4.987 (6.408)\tsimkgc 1.575 (3.522)\n","[2022-06-22 14:22:10,168 INFO] Epoch: [0][  800/34014]\tLoss 6.148 (9.846)\tInvT  19.97 ( 19.99)\tAcc@1  68.75 ( 31.53)\tAcc@3  93.75 ( 66.78)\tcake 4.919 (6.372)\tsimkgc 1.229 (3.473)\n","[2022-06-22 14:22:51,912 INFO] Epoch: [0][  820/34014]\tLoss 6.472 (9.77)\tInvT  19.97 ( 19.99)\tAcc@1  50.00 ( 32.19)\tAcc@3  93.75 ( 67.31)\tcake 5.065 (6.339)\tsimkgc 1.407 (3.431)\n","[2022-06-22 14:23:34,528 INFO] Epoch: [0][  840/34014]\tLoss 6.251 (9.7)\tInvT  19.97 ( 19.99)\tAcc@1  68.75 ( 32.77)\tAcc@3 100.00 ( 67.93)\tcake 4.975 (6.307)\tsimkgc 1.276 (3.392)\n","[2022-06-22 14:24:17,580 INFO] Epoch: [0][  860/34014]\tLoss 6.22 (9.629)\tInvT  19.97 ( 19.99)\tAcc@1  75.00 ( 33.32)\tAcc@3  93.75 ( 68.47)\tcake 5.132 (6.278)\tsimkgc 1.088 (3.351)\n","[2022-06-22 14:25:00,348 INFO] Epoch: [0][  880/34014]\tLoss 6.103 (9.556)\tInvT  19.97 ( 19.99)\tAcc@1  75.00 ( 33.92)\tAcc@3  87.50 ( 69.01)\tcake 4.955 (6.247)\tsimkgc 1.148 (3.309)\n","[2022-06-22 14:25:42,484 INFO] Epoch: [0][  900/34014]\tLoss 6.856 (9.488)\tInvT  19.97 ( 19.99)\tAcc@1  43.75 ( 34.52)\tAcc@3  93.75 ( 69.46)\tcake 4.837 (6.217)\tsimkgc 2.019 (3.272)\n","[2022-06-22 14:26:25,165 INFO] Epoch: [0][  920/34014]\tLoss 6.576 (9.424)\tInvT  19.97 ( 19.99)\tAcc@1  56.25 ( 35.04)\tAcc@3  87.50 ( 69.92)\tcake 5.08 (6.189)\tsimkgc 1.496 (3.235)\n","[2022-06-22 14:27:08,108 INFO] Epoch: [0][  940/34014]\tLoss 6.093 (9.361)\tInvT  19.97 ( 19.99)\tAcc@1  62.50 ( 35.59)\tAcc@3  93.75 ( 70.44)\tcake 4.755 (6.162)\tsimkgc 1.338 (3.198)\n","[2022-06-22 14:27:51,538 INFO] Epoch: [0][  960/34014]\tLoss 5.966 (9.3)\tInvT  19.97 ( 19.98)\tAcc@1  56.25 ( 36.11)\tAcc@3 100.00 ( 70.85)\tcake 4.869 (6.135)\tsimkgc 1.097 (3.165)\n","[2022-06-22 14:28:33,892 INFO] Epoch: [0][  980/34014]\tLoss 6.406 (9.242)\tInvT  19.96 ( 19.98)\tAcc@1  56.25 ( 36.54)\tAcc@3  93.75 ( 71.29)\tcake 4.744 (6.11)\tsimkgc 1.662 (3.133)\n","[2022-06-22 14:29:16,704 INFO] Epoch: [0][ 1000/34014]\tLoss 6.406 (9.186)\tInvT  19.96 ( 19.98)\tAcc@1  56.25 ( 36.99)\tAcc@3  87.50 ( 71.68)\tcake 4.677 (6.084)\tsimkgc 1.728 (3.102)\n","[2022-06-22 14:29:58,936 INFO] Epoch: [0][ 1020/34014]\tLoss 6.317 (9.132)\tInvT  19.96 ( 19.98)\tAcc@1  68.75 ( 37.41)\tAcc@3 100.00 ( 72.06)\tcake 4.807 (6.059)\tsimkgc 1.51 (3.073)\n","[2022-06-22 14:30:41,464 INFO] Epoch: [0][ 1040/34014]\tLoss 6.529 (9.083)\tInvT  19.96 ( 19.98)\tAcc@1  43.75 ( 37.78)\tAcc@3  93.75 ( 72.41)\tcake 4.831 (6.036)\tsimkgc 1.698 (3.046)\n","[2022-06-22 14:31:23,495 INFO] Epoch: [0][ 1060/34014]\tLoss 5.652 (9.037)\tInvT  19.96 ( 19.98)\tAcc@1  75.00 ( 38.10)\tAcc@3 100.00 ( 72.64)\tcake 4.868 (6.013)\tsimkgc 0.7844 (3.023)\n","[2022-06-22 14:32:05,656 INFO] Epoch: [0][ 1080/34014]\tLoss 5.934 (8.987)\tInvT  19.96 ( 19.98)\tAcc@1  56.25 ( 38.47)\tAcc@3 100.00 ( 73.03)\tcake 4.756 (5.991)\tsimkgc 1.178 (2.995)\n","[2022-06-22 14:32:49,133 INFO] Epoch: [0][ 1100/34014]\tLoss 6.007 (8.937)\tInvT  19.96 ( 19.98)\tAcc@1  50.00 ( 38.88)\tAcc@3 100.00 ( 73.40)\tcake 4.743 (5.97)\tsimkgc 1.265 (2.967)\n","[2022-06-22 14:33:31,722 INFO] Epoch: [0][ 1120/34014]\tLoss 6.165 (8.893)\tInvT  19.96 ( 19.98)\tAcc@1  50.00 ( 39.17)\tAcc@3  93.75 ( 73.66)\tcake 4.719 (5.947)\tsimkgc 1.446 (2.946)\n","[2022-06-22 14:34:14,943 INFO] Epoch: [0][ 1140/34014]\tLoss 6.564 (8.846)\tInvT  19.96 ( 19.98)\tAcc@1  62.50 ( 39.64)\tAcc@3 100.00 ( 73.99)\tcake 4.904 (5.926)\tsimkgc 1.66 (2.92)\n","[2022-06-22 14:34:57,254 INFO] Epoch: [0][ 1160/34014]\tLoss 5.751 (8.802)\tInvT  19.95 ( 19.98)\tAcc@1  56.25 ( 39.96)\tAcc@3 100.00 ( 74.30)\tcake 4.677 (5.904)\tsimkgc 1.074 (2.898)\n","[2022-06-22 14:35:40,192 INFO] Epoch: [0][ 1180/34014]\tLoss 5.729 (8.755)\tInvT  19.95 ( 19.98)\tAcc@1  75.00 ( 40.41)\tAcc@3 100.00 ( 74.66)\tcake 4.705 (5.885)\tsimkgc 1.024 (2.87)\n","[2022-06-22 14:36:22,328 INFO] Epoch: [0][ 1200/34014]\tLoss 6.564 (8.714)\tInvT  19.95 ( 19.98)\tAcc@1  50.00 ( 40.70)\tAcc@3  93.75 ( 74.97)\tcake 4.735 (5.866)\tsimkgc 1.83 (2.848)\n","[2022-06-22 14:37:05,707 INFO] Epoch: [0][ 1220/34014]\tLoss 6.67 (8.673)\tInvT  19.95 ( 19.98)\tAcc@1  50.00 ( 41.07)\tAcc@3  81.25 ( 75.22)\tcake 4.67 (5.847)\tsimkgc 2.0 (2.827)\n","[2022-06-22 14:37:48,309 INFO] Epoch: [0][ 1240/34014]\tLoss 5.944 (8.628)\tInvT  19.95 ( 19.98)\tAcc@1  68.75 ( 41.47)\tAcc@3  93.75 ( 75.54)\tcake 4.823 (5.827)\tsimkgc 1.121 (2.801)\n","[2022-06-22 14:38:32,380 INFO] Epoch: [0][ 1260/34014]\tLoss 5.724 (8.588)\tInvT  19.95 ( 19.98)\tAcc@1  75.00 ( 41.79)\tAcc@3  93.75 ( 75.84)\tcake 4.677 (5.809)\tsimkgc 1.047 (2.779)\n","[2022-06-22 14:39:14,916 INFO] Epoch: [0][ 1280/34014]\tLoss 5.452 (8.552)\tInvT  19.95 ( 19.98)\tAcc@1  81.25 ( 42.08)\tAcc@3 100.00 ( 76.10)\tcake 4.617 (5.792)\tsimkgc 0.8354 (2.76)\n","[2022-06-22 14:39:57,522 INFO] Epoch: [0][ 1300/34014]\tLoss 7.047 (8.516)\tInvT  19.95 ( 19.98)\tAcc@1  37.50 ( 42.38)\tAcc@3  93.75 ( 76.38)\tcake 4.843 (5.777)\tsimkgc 2.205 (2.74)\n","[2022-06-22 14:40:40,083 INFO] Epoch: [0][ 1320/34014]\tLoss 5.495 (8.481)\tInvT  19.95 ( 19.98)\tAcc@1  75.00 ( 42.72)\tAcc@3 100.00 ( 76.61)\tcake 4.615 (5.761)\tsimkgc 0.8795 (2.719)\n","[2022-06-22 14:41:22,885 INFO] Epoch: [0][ 1340/34014]\tLoss 6.637 (8.45)\tInvT  19.94 ( 19.98)\tAcc@1  50.00 ( 42.97)\tAcc@3  75.00 ( 76.75)\tcake 4.547 (5.745)\tsimkgc 2.09 (2.705)\n","[2022-06-22 14:42:04,842 INFO] Epoch: [0][ 1360/34014]\tLoss 6.105 (8.414)\tInvT  19.94 ( 19.98)\tAcc@1  62.50 ( 43.22)\tAcc@3 100.00 ( 76.99)\tcake 4.668 (5.729)\tsimkgc 1.437 (2.686)\n","[2022-06-22 14:42:47,303 INFO] Epoch: [0][ 1380/34014]\tLoss 6.237 (8.382)\tInvT  19.94 ( 19.98)\tAcc@1  68.75 ( 43.55)\tAcc@3 100.00 ( 77.25)\tcake 4.865 (5.714)\tsimkgc 1.372 (2.668)\n","[2022-06-22 14:43:31,717 INFO] Epoch: [0][ 1400/34014]\tLoss 6.275 (8.35)\tInvT  19.94 ( 19.97)\tAcc@1  56.25 ( 43.83)\tAcc@3  87.50 ( 77.48)\tcake 4.714 (5.699)\tsimkgc 1.561 (2.65)\n","[2022-06-22 14:44:14,715 INFO] Epoch: [0][ 1420/34014]\tLoss 5.084 (8.319)\tInvT  19.94 ( 19.97)\tAcc@1  87.50 ( 44.08)\tAcc@3 100.00 ( 77.66)\tcake 4.724 (5.684)\tsimkgc 0.3605 (2.635)\n","[2022-06-22 14:44:57,308 INFO] Epoch: [0][ 1440/34014]\tLoss 5.625 (8.288)\tInvT  19.94 ( 19.97)\tAcc@1  68.75 ( 44.34)\tAcc@3  87.50 ( 77.88)\tcake 4.703 (5.67)\tsimkgc 0.9223 (2.618)\n","[2022-06-22 14:45:39,956 INFO] Epoch: [0][ 1460/34014]\tLoss 6.403 (8.259)\tInvT  19.93 ( 19.97)\tAcc@1  50.00 ( 44.58)\tAcc@3  93.75 ( 78.08)\tcake 4.553 (5.656)\tsimkgc 1.851 (2.603)\n","[2022-06-22 14:46:22,408 INFO] Epoch: [0][ 1480/34014]\tLoss 5.435 (8.229)\tInvT  19.93 ( 19.97)\tAcc@1  68.75 ( 44.81)\tAcc@3 100.00 ( 78.30)\tcake 4.641 (5.643)\tsimkgc 0.7939 (2.586)\n","[2022-06-22 14:47:05,173 INFO] Epoch: [0][ 1500/34014]\tLoss 6.449 (8.202)\tInvT  19.93 ( 19.97)\tAcc@1  56.25 ( 45.01)\tAcc@3  93.75 ( 78.47)\tcake 4.54 (5.629)\tsimkgc 1.91 (2.573)\n","[2022-06-22 14:47:47,714 INFO] Epoch: [0][ 1520/34014]\tLoss 6.309 (8.174)\tInvT  19.93 ( 19.97)\tAcc@1  56.25 ( 45.29)\tAcc@3  81.25 ( 78.65)\tcake 4.577 (5.616)\tsimkgc 1.732 (2.558)\n","[2022-06-22 14:48:31,027 INFO] Epoch: [0][ 1540/34014]\tLoss 5.831 (8.147)\tInvT  19.93 ( 19.97)\tAcc@1  62.50 ( 45.51)\tAcc@3  93.75 ( 78.84)\tcake 4.736 (5.604)\tsimkgc 1.094 (2.544)\n","[2022-06-22 14:49:14,508 INFO] Epoch: [0][ 1560/34014]\tLoss 5.876 (8.121)\tInvT  19.93 ( 19.97)\tAcc@1  68.75 ( 45.71)\tAcc@3 100.00 ( 79.02)\tcake 4.822 (5.592)\tsimkgc 1.055 (2.529)\n","[2022-06-22 14:49:57,694 INFO] Epoch: [0][ 1580/34014]\tLoss 6.49 (8.1)\tInvT  19.93 ( 19.97)\tAcc@1  62.50 ( 45.84)\tAcc@3  93.75 ( 79.18)\tcake 4.804 (5.581)\tsimkgc 1.685 (2.519)\n","[2022-06-22 14:50:40,034 INFO] Epoch: [0][ 1600/34014]\tLoss 6.591 (8.074)\tInvT  19.93 ( 19.97)\tAcc@1  62.50 ( 46.12)\tAcc@3  81.25 ( 79.37)\tcake 4.978 (5.57)\tsimkgc 1.613 (2.503)\n","[2022-06-22 14:51:22,351 INFO] Epoch: [0][ 1620/34014]\tLoss 5.837 (8.049)\tInvT  19.92 ( 19.97)\tAcc@1  75.00 ( 46.36)\tAcc@3 100.00 ( 79.55)\tcake 4.626 (5.56)\tsimkgc 1.212 (2.489)\n","[2022-06-22 14:52:05,024 INFO] Epoch: [0][ 1640/34014]\tLoss 5.312 (8.025)\tInvT  19.92 ( 19.97)\tAcc@1  87.50 ( 46.59)\tAcc@3 100.00 ( 79.74)\tcake 4.59 (5.549)\tsimkgc 0.7221 (2.475)\n","[2022-06-22 14:52:47,771 INFO] Epoch: [0][ 1660/34014]\tLoss 6.365 (8.004)\tInvT  19.92 ( 19.97)\tAcc@1  62.50 ( 46.82)\tAcc@3  81.25 ( 79.88)\tcake 4.88 (5.54)\tsimkgc 1.485 (2.464)\n","[2022-06-22 14:53:29,800 INFO] Epoch: [0][ 1680/34014]\tLoss 6.279 (7.981)\tInvT  19.92 ( 19.97)\tAcc@1  56.25 ( 47.06)\tAcc@3  87.50 ( 80.05)\tcake 4.622 (5.53)\tsimkgc 1.658 (2.45)\n","[2022-06-22 14:54:13,799 INFO] Epoch: [0][ 1700/34014]\tLoss 5.274 (7.958)\tInvT  19.92 ( 19.97)\tAcc@1  87.50 ( 47.26)\tAcc@3 100.00 ( 80.19)\tcake 4.6 (5.52)\tsimkgc 0.6745 (2.438)\n","[2022-06-22 14:54:56,233 INFO] Epoch: [0][ 1720/34014]\tLoss 5.849 (7.933)\tInvT  19.92 ( 19.97)\tAcc@1  68.75 ( 47.51)\tAcc@3  93.75 ( 80.39)\tcake 4.544 (5.509)\tsimkgc 1.305 (2.423)\n","[2022-06-22 14:55:39,406 INFO] Epoch: [0][ 1740/34014]\tLoss 5.872 (7.91)\tInvT  19.92 ( 19.97)\tAcc@1  62.50 ( 47.69)\tAcc@3  87.50 ( 80.54)\tcake 4.255 (5.498)\tsimkgc 1.617 (2.411)\n","[2022-06-22 14:56:21,921 INFO] Epoch: [0][ 1760/34014]\tLoss 5.712 (7.886)\tInvT  19.91 ( 19.96)\tAcc@1  62.50 ( 47.93)\tAcc@3 100.00 ( 80.69)\tcake 4.454 (5.488)\tsimkgc 1.258 (2.399)\n","[2022-06-22 14:57:04,844 INFO] Epoch: [0][ 1780/34014]\tLoss 6.765 (7.864)\tInvT  19.91 ( 19.96)\tAcc@1  56.25 ( 48.13)\tAcc@3  75.00 ( 80.84)\tcake 4.648 (5.477)\tsimkgc 2.118 (2.387)\n","[2022-06-22 14:57:47,287 INFO] Epoch: [0][ 1800/34014]\tLoss 5.8 (7.841)\tInvT  19.91 ( 19.96)\tAcc@1  62.50 ( 48.34)\tAcc@3 100.00 ( 80.99)\tcake 4.753 (5.467)\tsimkgc 1.047 (2.374)\n","[2022-06-22 14:58:29,611 INFO] Epoch: [0][ 1820/34014]\tLoss 5.986 (7.818)\tInvT  19.91 ( 19.96)\tAcc@1  50.00 ( 48.56)\tAcc@3  93.75 ( 81.15)\tcake 4.549 (5.456)\tsimkgc 1.437 (2.362)\n","[2022-06-22 14:59:12,392 INFO] Epoch: [0][ 1840/34014]\tLoss 6.192 (7.799)\tInvT  19.91 ( 19.96)\tAcc@1  56.25 ( 48.73)\tAcc@3  93.75 ( 81.26)\tcake 4.48 (5.446)\tsimkgc 1.712 (2.353)\n","[2022-06-22 14:59:55,505 INFO] Epoch: [0][ 1860/34014]\tLoss 6.149 (7.779)\tInvT  19.91 ( 19.96)\tAcc@1  43.75 ( 48.90)\tAcc@3  93.75 ( 81.39)\tcake 4.338 (5.436)\tsimkgc 1.81 (2.343)\n","[2022-06-22 15:00:37,977 INFO] Epoch: [0][ 1880/34014]\tLoss 5.279 (7.757)\tInvT  19.91 ( 19.96)\tAcc@1  87.50 ( 49.09)\tAcc@3  93.75 ( 81.53)\tcake 4.575 (5.426)\tsimkgc 0.7048 (2.331)\n","[2022-06-22 15:01:19,569 INFO] Epoch: [0][ 1900/34014]\tLoss 5.53 (7.738)\tInvT  19.90 ( 19.96)\tAcc@1  62.50 ( 49.21)\tAcc@3 100.00 ( 81.63)\tcake 4.417 (5.416)\tsimkgc 1.112 (2.322)\n","[2022-06-22 15:02:01,652 INFO] Epoch: [0][ 1920/34014]\tLoss 6.589 (7.72)\tInvT  19.90 ( 19.96)\tAcc@1  62.50 ( 49.35)\tAcc@3  87.50 ( 81.73)\tcake 4.605 (5.407)\tsimkgc 1.984 (2.313)\n","[2022-06-22 15:02:43,154 INFO] Epoch: [0][ 1940/34014]\tLoss 5.571 (7.699)\tInvT  19.90 ( 19.96)\tAcc@1  68.75 ( 49.57)\tAcc@3 100.00 ( 81.89)\tcake 4.636 (5.398)\tsimkgc 0.9349 (2.301)\n","[2022-06-22 15:03:24,997 INFO] Epoch: [0][ 1960/34014]\tLoss 6.292 (7.682)\tInvT  19.90 ( 19.96)\tAcc@1  62.50 ( 49.71)\tAcc@3  87.50 ( 82.00)\tcake 4.85 (5.39)\tsimkgc 1.442 (2.292)\n","[2022-06-22 15:04:07,454 INFO] Epoch: [0][ 1980/34014]\tLoss 5.599 (7.664)\tInvT  19.90 ( 19.96)\tAcc@1  62.50 ( 49.88)\tAcc@3 100.00 ( 82.10)\tcake 4.543 (5.382)\tsimkgc 1.056 (2.282)\n","[2022-06-22 15:04:50,024 INFO] Epoch: [0][ 2000/34014]\tLoss 5.609 (7.647)\tInvT  19.90 ( 19.96)\tAcc@1  56.25 ( 50.03)\tAcc@3 100.00 ( 82.22)\tcake 4.549 (5.375)\tsimkgc 1.06 (2.272)\n","[2022-06-22 15:05:31,638 INFO] Epoch: [0][ 2020/34014]\tLoss 6.405 (7.632)\tInvT  19.89 ( 19.96)\tAcc@1  56.25 ( 50.14)\tAcc@3  87.50 ( 82.33)\tcake 4.64 (5.366)\tsimkgc 1.765 (2.265)\n","[2022-06-22 15:06:13,696 INFO] Epoch: [0][ 2040/34014]\tLoss 5.694 (7.615)\tInvT  19.89 ( 19.96)\tAcc@1  68.75 ( 50.31)\tAcc@3  93.75 ( 82.46)\tcake 4.536 (5.358)\tsimkgc 1.158 (2.257)\n","[2022-06-22 15:06:56,262 INFO] Epoch: [0][ 2060/34014]\tLoss 5.266 (7.598)\tInvT  19.89 ( 19.96)\tAcc@1  81.25 ( 50.47)\tAcc@3 100.00 ( 82.57)\tcake 4.714 (5.35)\tsimkgc 0.5515 (2.248)\n","[2022-06-22 15:07:38,587 INFO] Epoch: [0][ 2080/34014]\tLoss 5.618 (7.58)\tInvT  19.89 ( 19.96)\tAcc@1  68.75 ( 50.64)\tAcc@3  87.50 ( 82.68)\tcake 4.462 (5.343)\tsimkgc 1.157 (2.238)\n","[2022-06-22 15:08:20,629 INFO] Epoch: [0][ 2100/34014]\tLoss 5.387 (7.564)\tInvT  19.89 ( 19.95)\tAcc@1  81.25 ( 50.81)\tAcc@3  93.75 ( 82.80)\tcake 4.525 (5.335)\tsimkgc 0.8622 (2.229)\n","[2022-06-22 15:09:02,937 INFO] Epoch: [0][ 2120/34014]\tLoss 5.353 (7.548)\tInvT  19.89 ( 19.95)\tAcc@1  87.50 ( 50.95)\tAcc@3 100.00 ( 82.90)\tcake 4.578 (5.327)\tsimkgc 0.7748 (2.221)\n","[2022-06-22 15:09:45,353 INFO] Epoch: [0][ 2140/34014]\tLoss 5.231 (7.535)\tInvT  19.88 ( 19.95)\tAcc@1  87.50 ( 51.05)\tAcc@3 100.00 ( 83.01)\tcake 4.678 (5.321)\tsimkgc 0.5531 (2.214)\n","[2022-06-22 15:10:28,538 INFO] Epoch: [0][ 2160/34014]\tLoss 5.036 (7.521)\tInvT  19.88 ( 19.95)\tAcc@1  87.50 ( 51.16)\tAcc@3 100.00 ( 83.11)\tcake 4.659 (5.315)\tsimkgc 0.3771 (2.207)\n","[2022-06-22 15:11:09,646 INFO] Epoch: [0][ 2180/34014]\tLoss 5.474 (7.505)\tInvT  19.88 ( 19.95)\tAcc@1  68.75 ( 51.31)\tAcc@3 100.00 ( 83.23)\tcake 4.322 (5.308)\tsimkgc 1.151 (2.198)\n","[2022-06-22 15:11:50,939 INFO] Epoch: [0][ 2200/34014]\tLoss 5.855 (7.492)\tInvT  19.88 ( 19.95)\tAcc@1  56.25 ( 51.45)\tAcc@3  93.75 ( 83.32)\tcake 4.501 (5.301)\tsimkgc 1.354 (2.19)\n","[2022-06-22 15:12:32,544 INFO] Epoch: [0][ 2220/34014]\tLoss 5.869 (7.478)\tInvT  19.88 ( 19.95)\tAcc@1  68.75 ( 51.59)\tAcc@3 100.00 ( 83.42)\tcake 4.788 (5.295)\tsimkgc 1.082 (2.183)\n","[2022-06-22 15:13:14,917 INFO] Epoch: [0][ 2240/34014]\tLoss 6.012 (7.463)\tInvT  19.87 ( 19.95)\tAcc@1  75.00 ( 51.77)\tAcc@3  93.75 ( 83.51)\tcake 4.468 (5.288)\tsimkgc 1.543 (2.174)\n","[2022-06-22 15:13:57,038 INFO] Epoch: [0][ 2260/34014]\tLoss 6.165 (7.449)\tInvT  19.87 ( 19.95)\tAcc@1  62.50 ( 51.92)\tAcc@3  93.75 ( 83.61)\tcake 4.635 (5.282)\tsimkgc 1.53 (2.167)\n","[2022-06-22 15:14:39,739 INFO] Epoch: [0][ 2280/34014]\tLoss 6.451 (7.437)\tInvT  19.87 ( 19.95)\tAcc@1  56.25 ( 52.00)\tAcc@3  87.50 ( 83.67)\tcake 4.396 (5.275)\tsimkgc 2.055 (2.162)\n","[2022-06-22 15:15:22,005 INFO] Epoch: [0][ 2300/34014]\tLoss 6.065 (7.423)\tInvT  19.87 ( 19.95)\tAcc@1  75.00 ( 52.14)\tAcc@3  81.25 ( 83.76)\tcake 4.162 (5.268)\tsimkgc 1.903 (2.155)\n","[2022-06-22 15:16:04,078 INFO] Epoch: [0][ 2320/34014]\tLoss 6.651 (7.409)\tInvT  19.87 ( 19.95)\tAcc@1  56.25 ( 52.24)\tAcc@3  87.50 ( 83.82)\tcake 4.474 (5.261)\tsimkgc 2.178 (2.148)\n","[2022-06-22 15:16:47,294 INFO] Epoch: [0][ 2340/34014]\tLoss 5.587 (7.397)\tInvT  19.86 ( 19.95)\tAcc@1  81.25 ( 52.35)\tAcc@3 100.00 ( 83.93)\tcake 4.597 (5.254)\tsimkgc 0.99 (2.142)\n","[2022-06-22 15:17:29,499 INFO] Epoch: [0][ 2360/34014]\tLoss 6.596 (7.383)\tInvT  19.86 ( 19.95)\tAcc@1  62.50 ( 52.48)\tAcc@3  87.50 ( 84.02)\tcake 4.475 (5.249)\tsimkgc 2.121 (2.134)\n","[2022-06-22 15:18:11,662 INFO] Epoch: [0][ 2380/34014]\tLoss 4.749 (7.369)\tInvT  19.86 ( 19.95)\tAcc@1  87.50 ( 52.59)\tAcc@3 100.00 ( 84.12)\tcake 4.332 (5.242)\tsimkgc 0.4175 (2.127)\n","[2022-06-22 15:18:53,452 INFO] Epoch: [0][ 2400/34014]\tLoss 6.051 (7.356)\tInvT  19.86 ( 19.94)\tAcc@1  43.75 ( 52.68)\tAcc@3 100.00 ( 84.20)\tcake 4.446 (5.236)\tsimkgc 1.605 (2.121)\n"]}],"source":["!python -u /content/drive/MyDrive/CocaKE_ver4/main.py \\\n","--model-dir \"/content/drive/MyDrive/CocaKE_ver4/ouput\" \\\n","--pretrained-model bert-base-uncased \\\n","--pooling mean \\\n","--lr 1e-5 \\\n","--use-link-graph \\\n","--train-path \"/content/drive/MyDrive/CocaKE_ver4/data/FB15k237/train.txt.json\" \\\n","--valid-path \"/content/drive/MyDrive/CocaKE_ver4/data/FB15k237/valid.txt.json\" \\\n","--commonsense-path \"/content/drive/MyDrive/CocaKE_ver4/data/FB15k237\" \\\n","--head-ns-cnt 4 \\\n","--tail-ns-cnt 4 \\\n","--task FB15k237 \\\n","--batch-size 16 \\\n","--print-freq 20 \\\n","--additive-margin 0.02 \\\n","--use-amp \\\n","--use-self-negative \\\n","--finetune-t \\\n","--pre-batch 2 \\\n","--epochs 18 \\\n","--workers 4 \\\n","--max-to-keep 5 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8jSRm5rxxt2R","outputId":"8e1ff3f4-8a07-4976-f33e-4c45d50a1683"},"outputs":[{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"e:\\University\\Year 3 Spring\\Exchange\\ETH\\Lectures\\Computational Semantics\\Project\\CocaKE_Bruce\\CocaKE_ver4\\evaluate.py\", line 10, in <module>\n","    from config import args\n","  File \"e:\\University\\Year 3 Spring\\Exchange\\ETH\\Lectures\\Computational Semantics\\Project\\CocaKE_Bruce\\CocaKE_ver4\\config.py\", line 99, in <module>\n","    assert os.path.exists(args.eval_model_path), 'One of args.model_dir and args.eval_model_path should be valid path'\n","AssertionError: One of args.model_dir and args.eval_model_path should be valid path\n"]}],"source":["!python -u evaluate.py \\\n","--task FB15k237 \\\n","--is-test \\\n","--eval-model-path \"ouput/model_best.mdl\" \\\n","--neighbor-weight 0.05 \\\n","--rerank-n-hop 2 \\\n","--train-path \"data/FB15k237/train.txt.json\" \\\n","--valid-path \"data/FB15k237/test.txt.json\" "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-FE8xIpT_iS"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"CocaKE.ipynb","provenance":[]},"interpreter":{"hash":"91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}