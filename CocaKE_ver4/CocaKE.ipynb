{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8236,
     "status": "ok",
     "timestamp": 1653378015397,
     "user": {
      "displayName": "Bruce HU",
      "userId": "02309357718155453395"
     },
     "user_tz": -120
    },
    "id": "vZwq84Xtc1JK",
    "outputId": "1dab5771-2d2b-4a69-9ea5-28dfa329f4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.4.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 48.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 45.7 MB/s eta 0:00:01     |█████████▉                      | 2.0 MB 45.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Installing collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.7.0 regex-2022.4.24 tokenizers-0.12.1 transformers-4.19.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"e:\\University\\Year 3 Spring\\Exchange\\ETH\\Lectures\\Computational Semantics\\Project\\CocaKE_Bruce\\CocaKE_ver4\\preprocess.py\", line 22, in <module>\n",
      "    mp.set_start_method('fork')\n",
      "  File \"e:\\Anaconda\\lib\\multiprocessing\\context.py\", line 247, in set_start_method\n",
      "    self._actual_context = self.get_context(method)\n",
      "  File \"e:\\Anaconda\\lib\\multiprocessing\\context.py\", line 239, in get_context\n",
      "    return super().get_context(method)\n",
      "  File \"e:\\Anaconda\\lib\\multiprocessing\\context.py\", line 193, in get_context\n",
      "    raise ValueError('cannot find context for %r' % method) from None\n",
      "ValueError: cannot find context for 'fork'\n"
     ]
    }
   ],
   "source": [
    "!python -u preprocess.py --task \"FB15k237\" \\\n",
    "--train-path \"data/FB15k237/train.txt\" \\\n",
    "--valid-path \"data/FB15k237/valid.txt\" \\\n",
    "--test-path \"data/FB15k237/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1652736004094,
     "user": {
      "displayName": "Bruce HU",
      "userId": "02309357718155453395"
     },
     "user_tz": -120
    },
    "id": "fsHAPIyscjw_",
    "outputId": "4bd64e57-cb84-4f99-c0be-95e94e4e4d00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 10:53:40.388027: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-06-17 10:53:40.388542: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-06-17 10:53:54,012 INFO] Load 14541 entities from data/FB15k237\\entities.json\n",
      "[2022-06-17 10:53:54,012 INFO] Triplets path: ['data/FB15k237/train.txt.json']\n",
      "[2022-06-17 10:53:55,600 INFO] Triplet statistics: 474 relations, 544230 triplets\n",
      "[2022-06-17 10:53:55,600 INFO] Start to build link graph from data/FB15k237/train.txt.json\n",
      "[2022-06-17 10:53:56,318 INFO] Done build link graph with 14505 nodes\n",
      "[2022-06-17 10:53:56,373 INFO] Use 1 gpus for training\n",
      "[2022-06-17 10:54:00,194 INFO] Build tokenizer from bert-base-uncased\n",
      "[2022-06-17 10:54:00,194 INFO] => creating model\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2022-06-17 10:54:02,941 INFO] CustomBertModel(\n",
      "  (hr_bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (tail_bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "e:\\Anaconda\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2022-06-17 10:54:05,929 INFO] log_inv_t: 1.0\n",
      "[2022-06-17 10:54:05,930 INFO] hr_bert.embeddings.word_embeddings.weight: 23440896\n",
      "[2022-06-17 10:54:05,930 INFO] hr_bert.embeddings.position_embeddings.weight: 393216\n",
      "[2022-06-17 10:54:05,930 INFO] hr_bert.embeddings.token_type_embeddings.weight: 1536\n",
      "[2022-06-17 10:54:05,930 INFO] hr_bert.embeddings.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,930 INFO] hr_bert.embeddings.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,930 INFO] hr_bert.encoder.layer.0.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,930 INFO] hr_bert.encoder.layer.0.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,930 INFO] hr_bert.encoder.layer.0.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,930 INFO] hr_bert.encoder.layer.0.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,930 INFO] hr_bert.encoder.layer.0.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.0.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.1.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.1.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.1.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.1.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,931 INFO] hr_bert.encoder.layer.1.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.1.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.2.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.2.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.2.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.2.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,932 INFO] hr_bert.encoder.layer.2.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.2.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.3.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.3.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.3.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.3.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,933 INFO] hr_bert.encoder.layer.3.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.3.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.4.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.4.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.4.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.4.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,934 INFO] hr_bert.encoder.layer.4.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.4.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.5.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.5.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.5.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.5.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,935 INFO] hr_bert.encoder.layer.5.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.5.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.6.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,936 INFO] hr_bert.encoder.layer.6.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.6.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.7.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,937 INFO] hr_bert.encoder.layer.7.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.7.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.8.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,938 INFO] hr_bert.encoder.layer.8.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.8.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.9.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,939 INFO] hr_bert.encoder.layer.9.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.9.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,940 INFO] hr_bert.encoder.layer.10.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.10.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,941 INFO] hr_bert.encoder.layer.11.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.encoder.layer.11.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,942 INFO] hr_bert.pooler.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,943 INFO] hr_bert.pooler.dense.bias: 768\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.embeddings.word_embeddings.weight: 23440896\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.embeddings.position_embeddings.weight: 393216\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.embeddings.token_type_embeddings.weight: 1536\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.embeddings.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.embeddings.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.encoder.layer.0.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.encoder.layer.0.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.encoder.layer.0.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.encoder.layer.0.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.encoder.layer.0.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.encoder.layer.0.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.encoder.layer.0.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.encoder.layer.0.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,943 INFO] tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.0.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.0.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.0.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.0.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.0.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.1.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.1.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.1.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.1.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.1.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.1.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.1.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.1.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,944 INFO] tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.1.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.1.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.1.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.1.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.1.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.2.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.2.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.2.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.2.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.2.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.2.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.2.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,945 INFO] tail_bert.encoder.layer.2.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.2.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.2.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.2.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.2.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.2.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.3.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.3.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.3.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.3.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.3.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.3.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.3.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,946 INFO] tail_bert.encoder.layer.3.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.3.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.3.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.3.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.3.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.3.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.4.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.4.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.4.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.4.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.4.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.4.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.4.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,947 INFO] tail_bert.encoder.layer.4.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.4.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.4.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.4.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.4.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.4.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.5.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.5.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.5.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.5.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.5.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.5.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,948 INFO] tail_bert.encoder.layer.5.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.5.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.5.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.5.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.5.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.5.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.5.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.6.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.6.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.6.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.6.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.6.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,949 INFO] tail_bert.encoder.layer.6.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.6.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.6.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.6.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.6.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.6.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.6.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.6.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.7.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.7.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.7.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.7.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.7.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,950 INFO] tail_bert.encoder.layer.7.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.7.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.7.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.7.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.7.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.7.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.7.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.7.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.8.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.8.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.8.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.8.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,951 INFO] tail_bert.encoder.layer.8.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.8.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.9.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.9.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.9.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,952 INFO] tail_bert.encoder.layer.9.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.9.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.10.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.10.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.10.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,953 INFO] tail_bert.encoder.layer.10.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.attention.output.dense.bias: 768"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.10.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.11.attention.self.query.weight: 589824\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.11.attention.self.query.bias: 768\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.11.attention.self.key.weight: 589824\n",
      "[2022-06-17 10:54:05,954 INFO] tail_bert.encoder.layer.11.attention.self.key.bias: 768\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.attention.self.value.weight: 589824\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.attention.self.value.bias: 768\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.attention.output.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.attention.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.intermediate.dense.bias: 3072\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.output.dense.weight: 2359296\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.output.dense.bias: 768\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.output.LayerNorm.weight: 768\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.encoder.layer.11.output.LayerNorm.bias: 768\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.pooler.dense.weight: 589824\n",
      "[2022-06-17 10:54:05,955 INFO] tail_bert.pooler.dense.bias: 768\n",
      "[2022-06-17 10:54:05,956 INFO] Number of parameters: 218.0M\n",
      "[2022-06-17 10:54:05,984 INFO] In test mode: False\n",
      "[2022-06-17 10:54:06,400 INFO] Load 272115 examples from data/FB15k237/train.txt.json\n",
      "[2022-06-17 10:54:07,775 INFO] In test mode: False\n",
      "[2022-06-17 10:54:07,819 INFO] Load 17535 examples from data/FB15k237/valid.txt.json\n",
      "[2022-06-17 10:54:07,867 INFO] Total training steps: 4898070, warmup steps: 400\n",
      "[2022-06-17 10:54:07,868 INFO] Args={\n",
      "    \"pretrained_model\": \"bert-base-uncased\",\n",
      "    \"task\": \"FB15k237\",\n",
      "    \"train_path\": \"data/FB15k237/train.txt.json\",\n",
      "    \"valid_path\": \"data/FB15k237/valid.txt.json\",\n",
      "    \"model_dir\": \"ouput\",\n",
      "    \"warmup\": 400,\n",
      "    \"max_to_keep\": 5,\n",
      "    \"grad_clip\": 10.0,\n",
      "    \"pooling\": \"mean\",\n",
      "    \"dropout\": 0.1,\n",
      "    \"use_amp\": true,\n",
      "    \"t\": 0.05,\n",
      "    \"use_link_graph\": true,\n",
      "    \"eval_every_n_step\": 10000,\n",
      "    \"pre_batch\": 2,\n",
      "    \"pre_batch_weight\": 0.5,\n",
      "    \"additive_margin\": 0.02,\n",
      "    \"finetune_t\": true,\n",
      "    \"max_num_tokens\": 50,\n",
      "    \"use_self_negative\": true,\n",
      "    \"commonsense_path\": \"data/FB15k237\",\n",
      "    \"head_ns_cnt\": 1,\n",
      "    \"tail_ns_cnt\": 1,\n",
      "    \"workers\": 4,\n",
      "    \"epochs\": 18,\n",
      "    \"batch_size\": 2,\n",
      "    \"lr\": 1e-05,\n",
      "    \"lr_scheduler\": \"linear\",\n",
      "    \"weight_decay\": 0.0001,\n",
      "    \"print_freq\": 20,\n",
      "    \"seed\": null,\n",
      "    \"is_test\": false,\n",
      "    \"rerank_n_hop\": 2,\n",
      "    \"neighbor_weight\": 0.0,\n",
      "    \"eval_model_path\": \"\",\n",
      "    \"cake_gamma\": 9.0,\n",
      "    \"cake_adversial_weight\": 10.0\n",
      "}\n",
      "2022-06-17 10:54:10.473893: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-06-17 10:54:10.474278: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-06-17 10:54:15,201 INFO] Load 14541 entities from data/FB15k237\\entities.json\n",
      "[2022-06-17 10:54:15,202 INFO] Triplets path: ['data/FB15k237/train.txt.json']\n",
      "[2022-06-17 10:54:16,739 INFO] Triplet statistics: 474 relations, 544230 triplets\n",
      "[2022-06-17 10:54:16,739 INFO] Start to build link graph from data/FB15k237/train.txt.json\n",
      "[2022-06-17 10:54:17,419 INFO] Done build link graph with 14505 nodes\n",
      "2022-06-17 10:54:21.866065: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-06-17 10:54:21.866409: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-06-17 10:54:26,650 INFO] Load 14541 entities from data/FB15k237\\entities.json\n",
      "[2022-06-17 10:54:26,650 INFO] Triplets path: ['data/FB15k237/train.txt.json']\n",
      "[2022-06-17 10:54:28,106 INFO] Triplet statistics: 474 relations, 544230 triplets\n",
      "[2022-06-17 10:54:28,106 INFO] Start to build link graph from data/FB15k237/train.txt.json\n",
      "[2022-06-17 10:54:28,800 INFO] Done build link graph with 14505 nodes\n",
      "2022-06-17 10:54:33.330891: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-06-17 10:54:33.331320: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-06-17 10:54:38,231 INFO] Load 14541 entities from data/FB15k237\\entities.json\n",
      "[2022-06-17 10:54:38,231 INFO] Triplets path: ['data/FB15k237/train.txt.json']\n",
      "[2022-06-17 10:54:39,696 INFO] Triplet statistics: 474 relations, 544230 triplets\n",
      "[2022-06-17 10:54:39,696 INFO] Start to build link graph from data/FB15k237/train.txt.json\n",
      "[2022-06-17 10:54:40,400 INFO] Done build link graph with 14505 nodes\n",
      "2022-06-17 10:54:45.048506: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-06-17 10:54:45.048931: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[2022-06-17 10:54:50,810 INFO] Load 14541 entities from data/FB15k237\\entities.json\n",
      "[2022-06-17 10:54:50,810 INFO] Triplets path: ['data/FB15k237/train.txt.json']\n",
      "[2022-06-17 10:54:52,314 INFO] Triplet statistics: 474 relations, 544230 triplets\n",
      "[2022-06-17 10:54:52,314 INFO] Start to build link graph from data/FB15k237/train.txt.json\n",
      "[2022-06-17 10:54:53,025 INFO] Done build link graph with 14505 nodes\n",
      "[2022-06-17 10:54:58,694 INFO] Build tokenizer from bert-base-uncased\n",
      "[2022-06-17 10:54:58,716 INFO] Build tokenizer from bert-base-uncased\n",
      "[2022-06-17 10:54:58,721 INFO] Build tokenizer from bert-base-uncased\n",
      "[2022-06-17 10:54:58,742 INFO] Build tokenizer from bert-base-uncased\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\University\\Year 3 Spring\\Exchange\\ETH\\Lectures\\Computational Semantics\\Project\\CocaKE_Bruce\\CocaKE_ver4\\main.py\", line 36, in <module>\n",
      "    main()\n",
      "  File \"e:\\University\\Year 3 Spring\\Exchange\\ETH\\Lectures\\Computational Semantics\\Project\\CocaKE_Bruce\\CocaKE_ver4\\main.py\", line 19, in main\n",
      "    trainer.train_loop()\n",
      "  File \"e:\\University\\Year 3 Spring\\Exchange\\ETH\\Lectures\\Computational Semantics\\Project\\CocaKE_Bruce\\CocaKE_ver4\\trainer.py\", line 76, in train_loop\n",
      "    self.train_epoch(epoch)\n",
      "  File \"e:\\University\\Year 3 Spring\\Exchange\\ETH\\Lectures\\Computational Semantics\\Project\\CocaKE_Bruce\\CocaKE_ver4\\trainer.py\", line 155, in train_epoch\n",
      "    cake_loss = get_model_obj(self.model).compute_cake_loss(simkgc_output=outputs[\"simkgc\"], cake_output_dict= outputs[\"cake\"])\n",
      "  File \"e:\\University\\Year 3 Spring\\Exchange\\ETH\\Lectures\\Computational Semantics\\Project\\CocaKE_Bruce\\CocaKE_ver4\\models.py\", line 108, in compute_cake_loss\n",
      "    positive_score -= torch.zeros(positive_score.size()).fill_diagonal_(self.cake_gamma).to(positive_score.device)\n",
      "RuntimeError: dimensions must larger than 1\n"
     ]
    }
   ],
   "source": [
    "!python -u main.py \\\n",
    "--model-dir \"ouput\" \\\n",
    "--pretrained-model bert-base-uncased \\\n",
    "--pooling mean \\\n",
    "--lr 1e-5 \\\n",
    "--use-link-graph \\\n",
    "--train-path \"data/FB15k237/train.txt.json\" \\\n",
    "--valid-path \"data/FB15k237/valid.txt.json\" \\\n",
    "--commonsense-path \"data/FB15k237\" \\\n",
    "--head-ns-cnt 1 \\\n",
    "--tail-ns-cnt 1 \\\n",
    "--task FB15k237 \\\n",
    "--batch-size 2 \\\n",
    "--print-freq 20 \\\n",
    "--additive-margin 0.02 \\\n",
    "--use-amp \\\n",
    "--use-self-negative \\\n",
    "--finetune-t \\\n",
    "--pre-batch 2 \\\n",
    "--epochs 18 \\\n",
    "--workers 4 \\\n",
    "--max-to-keep 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"e:\\University\\Year 3 Spring\\Exchange\\ETH\\Lectures\\Computational Semantics\\Project\\CocaKE_Bruce\\CocaKE_ver4\\evaluate.py\", line 10, in <module>\n",
      "    from config import args\n",
      "  File \"e:\\University\\Year 3 Spring\\Exchange\\ETH\\Lectures\\Computational Semantics\\Project\\CocaKE_Bruce\\CocaKE_ver4\\config.py\", line 99, in <module>\n",
      "    assert os.path.exists(args.eval_model_path), 'One of args.model_dir and args.eval_model_path should be valid path'\n",
      "AssertionError: One of args.model_dir and args.eval_model_path should be valid path\n"
     ]
    }
   ],
   "source": [
    "!python -u evaluate.py \\\n",
    "--task FB15k237 \\\n",
    "--is-test \\\n",
    "--eval-model-path \"ouput/model_best.mdl\" \\\n",
    "--neighbor-weight 0.05 \\\n",
    "--rerank-n-hop 2 \\\n",
    "--train-path \"data/FB15k237/train.txt.json\" \\\n",
    "--valid-path \"data/FB15k237/test.txt.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-FE8xIpT_iS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CocaKE.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
